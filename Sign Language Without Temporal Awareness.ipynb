{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f9c5dca",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "043da368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import os,sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0febc5",
   "metadata": {},
   "source": [
    "# Keypoints using MP Holistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b750ce",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3f22093",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7097472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5ce76a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image,results.face_landmarks, mp_holistic.FACEMESH_TESSELATION)\n",
    "    mp_drawing.draw_landmarks(image,results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(image,results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(image,results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fb3c16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image,results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
    "                             mp_drawing.DrawingSpec(color=(43,180,255), thickness = 1,circle_radius=2),\n",
    "                             mp_drawing.DrawingSpec(color=(255,0,255), thickness = 1,circle_radius=1)\n",
    "                             )\n",
    "    mp_drawing.draw_landmarks(image,results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(43,180,255), thickness = 2,circle_radius=3),\n",
    "                             mp_drawing.DrawingSpec(color=(255,0,255), thickness = 3,circle_radius=3)\n",
    "                             )\n",
    "    mp_drawing.draw_landmarks(image,results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(43,180,255), thickness = 2,circle_radius=3),\n",
    "                             mp_drawing.DrawingSpec(color=(255,0,255), thickness = 2,circle_radius=1)\n",
    "                             )\n",
    "    mp_drawing.draw_landmarks(image,results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(43,180,255), thickness = 2,circle_radius=3),\n",
    "                             mp_drawing.DrawingSpec(color=(255,0,255), thickness = 2,circle_radius=1)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb9a6cd9-8e4d-49be-a638-8e35570b0251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks_np(image, results):\n",
    "    mp_drawing.draw_landmarks(image,results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
    "                             mp_drawing.DrawingSpec(color=(43,180,255), thickness = 1,circle_radius=2),\n",
    "                             mp_drawing.DrawingSpec(color=(255,0,255), thickness = 1,circle_radius=1)\n",
    "                             )\n",
    "    mp_drawing.draw_landmarks(image,results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(43,180,255), thickness = 2,circle_radius=3),\n",
    "                             mp_drawing.DrawingSpec(color=(255,0,255), thickness = 2,circle_radius=1)\n",
    "                             )\n",
    "    mp_drawing.draw_landmarks(image,results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(43,180,255), thickness = 2,circle_radius=3),\n",
    "                             mp_drawing.DrawingSpec(color=(255,0,255), thickness = 2,circle_radius=1)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee508f9",
   "metadata": {},
   "source": [
    "### Detecting from an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd112427",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter picture input/ file path\n",
    "rawpic = os.path.join(\"Guardian Demon 18.png\")\n",
    "\n",
    "#convert to a numpy array\n",
    "pic = cv2.imread(rawpic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fed2c1f2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.5) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-25bd487a759a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#make detections\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mpicpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmediapipe_detection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mholistic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#draw landmarks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-73888fbd0fdd>\u001b[0m in \u001b[0;36mmediapipe_detection\u001b[1;34m(image, model)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmediapipe_detection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriteable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriteable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.5) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "#set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    #make detections\n",
    "    picpr, results = mediapipe_detection(pic, holistic)\n",
    "    \n",
    "    #draw landmarks\n",
    "    #draw_landmarks(image, results)\n",
    "    draw_styled_landmarks(picpr, results)\n",
    "    \n",
    "    #display image\n",
    "    plt.imshow(cv2.cvtColor(picpr, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5e7d0f4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save output as jpeg\n",
    "cv2.imwrite(\"out.jpg\", picpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca9b63d",
   "metadata": {},
   "source": [
    "### Detecting from Webcam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecc0bad-59bc-4478-a588-e03088e547e7",
   "metadata": {},
   "source": [
    "#### Overlayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7998ac66-e297-42c2-a50f-cda77e712165",
   "metadata": {},
   "source": [
    "##### laptop webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c0f71490",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "#set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        #read frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        #make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "               \n",
    "        #draw landmarks\n",
    "        #draw_landmarks(image, results)\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        #show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        \n",
    "        #break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF ==ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b24dd1-daae-408b-aa8a-68a7009dac12",
   "metadata": {},
   "source": [
    "##### IP webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "07131198-5f6a-4fc5-b5e0-3c0dc1ecf608",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"http://192.168.1.36:8080/video\")\n",
    "\n",
    "#Resizing feed\n",
    "scale_percent = 40 # percent of original size\n",
    "width = int(image.shape[1] * scale_percent / 100)\n",
    "height = int(image.shape[0] * scale_percent / 100)\n",
    "dim = (width, height)\n",
    "\n",
    "#set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        #read frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        #make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "               \n",
    "        #draw landmarks\n",
    "        #draw_landmarks(image, results)\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        #Resizing\n",
    "        #resized = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "        resized = cv2.resize(image, dim)\n",
    "        \n",
    "        #Flipping\n",
    "        im_flip = cv2.rotate(resized,0)\n",
    "        \n",
    "        #show to screen\n",
    "        cv2.imshow('OpenCV Feed', im_flip)\n",
    "        \n",
    "        #break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF ==ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd3b6c1-a949-4d52-acef-5ea656c38e9f",
   "metadata": {},
   "source": [
    "#### Black background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79a3f1a9-130e-4434-8583-84b42131eb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First large number is the number of pixels in the columns or width\n",
    "frame1 = np.full((640,3),0)\n",
    "# the *160 is the number of pixels in the rows/ height divided by 3\n",
    "frame2 = np.array([frame1,frame1,frame1]*160,dtype=np.uint8)\n",
    "\n",
    "cap = cv2.VideoCapture(\"http://192.168.43.52:8080/video\")\n",
    "#set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        #read frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        #make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        \n",
    "        #style stuff, change frame2 to image from draw landmarks below for overlayed output\n",
    "        frame2 = np.array([frame1,frame1,frame1]*160,dtype=np.uint8)\n",
    "               \n",
    "        #draw landmarks\n",
    "        #draw_landmarks(image, results)\n",
    "        draw_styled_landmarks(frame2, results)\n",
    "\n",
    "        #show to screen\n",
    "        cv2.imshow('OpenCV Feed', frame2)\n",
    "        \n",
    "        #break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF ==ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a0d325",
   "metadata": {},
   "source": [
    "### Detecting from a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f8c5425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter video input/ file path\n",
    "video = \"signtest.avi\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f7eb1c-4506-4e3c-9908-147436f5d17b",
   "metadata": {},
   "source": [
    "#### Overlayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1e3d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    #Establish capture\n",
    "    cap = cv2.VideoCapture(os.path.join(video))\n",
    "    \n",
    "    #Setup Video writer\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    #Videowriter\n",
    "    video_writer = cv2.VideoWriter(os.path.join('output3.mp4'), cv2.VideoWriter_fourcc('M','P','4','2'), fps, (width, height))\n",
    "    \n",
    "    # Loop through each frame\n",
    "    for frame_idx in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))):\n",
    "        #read frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        #make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "        #draw landmarks\n",
    "        #draw_landmarks(image, results)\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        #show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        \n",
    "        #Write out frame\n",
    "        #video_writer.write(image)\n",
    "\n",
    "        #Breaking the loop\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    #close down everything\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    video_writer.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b70d2c-11f2-4af4-9f1b-04fabb0329bf",
   "metadata": {},
   "source": [
    "#### Black Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "feba89d8-1645-422e-9358-2dc3ec7230d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    #Establish capture\n",
    "    cap = cv2.VideoCapture(os.path.join(video))\n",
    "    \n",
    "    #Setup Video writer\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    frame1 = np.full((width,3),0)\n",
    "        \n",
    "    #Videowriter\n",
    "    video_writer = cv2.VideoWriter(os.path.join('signttestOutput.mp4'), cv2.VideoWriter_fourcc('P','I','M','1'), fps, (width, height))\n",
    "    \n",
    "    # Loop through each frame\n",
    "    for frame_idx in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))):\n",
    "        #read frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        #make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        \n",
    "        frame2 = np.array([frame1,frame1,frame1]*int(height/3),dtype=np.uint8)\n",
    "\n",
    "        #draw landmarks\n",
    "        #draw_landmarks(image, results)\n",
    "        draw_styled_landmarks(frame2, results)\n",
    "        #draw_styled_landmarks_np(frame2, results)\n",
    "\n",
    "        #show to screen\n",
    "        cv2.imshow('OpenCV Feed', frame2)\n",
    "        \n",
    "        #Write out frame\n",
    "        #video_writer.write(frame2)\n",
    "\n",
    "        #Breaking the loop\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    #close down everything\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    video_writer.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb08556",
   "metadata": {},
   "source": [
    "## Extracting keypoint values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c881e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh =np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0b288d",
   "metadata": {},
   "source": [
    "## Setup Folders for Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a568f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_DATA')\n",
    "\n",
    "#Actions that we are trying to deetect\n",
    "#actions = np.array([ ' A', ' B', ' C', ' D', ' E', ' F', ' G', ' H', ' I', ' J', ' K', ' L', ' M', ' N', ' O', ' P', ' Q', ' R', ' S', ' T', ' U', ' V', ' W', ' X', ' Y', ' Z', 'Hello', 'my', 'name'])\n",
    "actions = np.array([ 'Hello', 'my', 'name'])\n",
    "\n",
    "#Thirty videos worth of data\n",
    "no_sequences = 30\n",
    "\n",
    "#videos are going to be 30 frames in length\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f6049bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce4048c",
   "metadata": {},
   "source": [
    "## Collect Keypoint Values for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d68269ad",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\MARKMA~1\\AppData\\Local\\Temp/ipykernel_13536/741992133.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                 \u001b[1;31m#make detections\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                 \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmediapipe_detection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mholistic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[1;31m#draw landmarks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\MARKMA~1\\AppData\\Local\\Temp/ipykernel_13536/3319747627.py\u001b[0m in \u001b[0;36mmediapipe_detection\u001b[1;34m(image, model)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriteable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriteable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_RGB2BGR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\playground\\lib\\site-packages\\mediapipe\\python\\solutions\\holistic.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \"\"\"\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'image'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpose_landmarks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mlandmark\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpose_landmarks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlandmark\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\playground\\lib\\site-packages\\mediapipe\\python\\solution_base.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    332\u001b[0m                                      data).at(self._simulated_timestamp))\n\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_until_idle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m     \u001b[1;31m# Create a NamedTuple object where the field names are mapping to the graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[1;31m# output stream names.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "#set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    #loop through actions\n",
    "    for action in actions:\n",
    "        #loop through videos\n",
    "        for sequence in range(no_sequences):\n",
    "            #loop through video length aka sequence length\n",
    "            for frame_num in range(sequence_length):\n",
    "                \n",
    "                #read frame\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                #make detections\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "                #draw landmarks\n",
    "                #draw_landmarks(image, results)\n",
    "                draw_styled_landmarks(image, results)\n",
    "                \n",
    "                #apply wait logic\n",
    "                if frame_num == 0:\n",
    "                    cv2.putText(image,'STARTING COLLECTION',(120,200),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 4,cv2.LINE_AA)\n",
    "                    cv2.waitKey(2000)\n",
    "                    cv2.putText(image,'Collecting frames for {} Video Number{}'.format(action, sequence),(15,12),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1,cv2.LINE_AA)\n",
    "                    cv2.waitKey(2000)\n",
    "                else:\n",
    "                    cv2.putText(image,'Collecting frames for {} Video Number{}'.format(action, sequence),(15,12),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1,cv2.LINE_AA)\n",
    "                    \n",
    "                #Emport keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence),str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                #show to screen\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "                #break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF ==ord('q'):\n",
    "                    break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c2eb21-bf83-4340-9e59-ae48985612fa",
   "metadata": {},
   "source": [
    "## Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "414caff0-514a-4052-9723-dfabd0da21a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb83319f-dc50-445e-a69c-cff67278458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bb7a77-b811-4fd5-bcee-13b9ffbbd0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action,str(sequence),\"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27375e47-9aea-4019-9bdd-634fc4266a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a934769-9501-48c6-81e1-fbe9bcd60396",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf1c263-6fb7-4d36-916c-f15e5406cef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IBMAI2",
   "language": "python",
   "name": "ibmai2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
